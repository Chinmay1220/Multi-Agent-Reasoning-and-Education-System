# üìò Teaching Multi-Agent LLM Systems: Planner‚ÄìRetriever‚ÄìAnalyst‚ÄìCritic

This project is my **take-home final for INFO 7390: Advanced Data Science and Architecture**.  
It is a fully functioning **multi-agent LLM teaching system** designed to show *how multiple AI agents collaborate to answer questions more reliably, transparently, and interpretably than a single LLM*.

The goal of this project is both **technical** (building a modular multi-agent architecture) and **pedagogical** (teaching AI reasoning, Botspeak roles, GIGO, computational skepticism, and retrieval-augmented workflows).

---

# üöÄ Project Overview

This project demonstrates a four-agent architecture:

- **Planner** ‚Äî Clarifies the user‚Äôs question and breaks it into subtasks  
- **RetrieverAgent** ‚Äî Performs embedding-based retrieval over a small teaching corpus  
- **Analyst** ‚Äî Synthesizes a grounded answer using retrieved evidence  
- **Critic** ‚Äî Evaluates the draft answer, identifies hallucinations, and provides a corrected final answer  

Instead of generating a single opaque LLM response, the system makes every step **visible, inspectable, and teachable**.  
This is achieved through:

- A **Streamlit UI** that exposes intermediate agent steps  
- **Teaching notebooks** that walk from single-agent ‚Üí two-agent ‚Üí full four-agent systems  
- **A tutorial document** explaining concepts and workflows  
- **Debugging scenarios** to teach failure cases  
- **A Show-and-Tell video** following the Explain ‚Üí Show ‚Üí Try method  

This structure creates an **X-ray view of LLM reasoning**, making it ideal for learning, demonstration, and interviews.

---

# üîç Concept Overview

Multi-agent LLM systems decompose a complex task into smaller, focused roles.  
Rather than depending on one large prompt, this approach uses **agent specialization**:

- Improves reliability  
- Makes reasoning interpretable  
- Supports grounded evidence-based answers  
- Reduces hallucinations  
- Aligns with INFO 7390 themes like:
  - **Botspeak Framework**
  - **GIGO: Garbage In, Garbage Out**
  - **Computational Skepticism**
  - **Structured prompting**
  - **Retrieval-Augmented Reasoning**

This project implements a clean, modular **Planner ‚Üí RetrieverAgent ‚Üí Analyst ‚Üí Critic** pipeline over a small corpus of instructional text files.

---

# üéØ Learning Objectives

By exploring this project, a student will be able to:

### Core Understanding
‚úî Explain what a multi-agent LLM system is  
‚úî Understand why task decomposition improves reliability  
‚úî Interpret evidence-backed answer generation  
‚úî Recognize how hallucinations arise and how agents reduce them  

### Implementation Skills
‚úî Implement a multi-agent pipeline in Python  
‚úî Write agent prompts inspired by Botspeak  
‚úî Use embeddings for simple retrieval  
‚úî Build and evaluate agent reasoning chains  

### Debugging / Extension
‚úî Diagnose failure cases  
‚úî Explore GIGO effects through retrieval quality  
‚úî Modify agent prompts  
‚úî Extend the system with new roles (e.g., Style Editor, Explainer, Verifier)  

---

# üß† How the System Works ‚Äî Step-by-Step

Below is the full reasoning pipeline executed for each user question:

---

## 1Ô∏è‚É£ User Question  
The user types a question into the Streamlit app (or CLI).  
Example:  
> ‚ÄúWhy might multi-agent LLM systems be more reliable than a single agent?‚Äù

---

## 2Ô∏è‚É£ Planner Agent ‚Äî *Clarify & Decompose*  
The Planner agent:  
- Clarifies the question  
- Rewrites it if necessary  
- Breaks it into smaller, concrete subtasks  

Example output:

```json
{
  "clarified_question": "Explain why using multiple specialized LLM agents can be more reliable than a single agent.",
  "subtasks": [
    "Define what a multi-agent LLM system is.",
    "Identify typical agent roles.",
    "Compare multi-agent workflows to single-agent workflows.",
    "Explain how they reduce hallucinations."
  ]
}
